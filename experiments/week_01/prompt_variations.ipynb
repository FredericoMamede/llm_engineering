{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Scraper Optimization: Function vs Class-Based Approach\n",
                "\n",
                "**Problem:** Original implementation parses HTML twice (2 HTTP requests, duplicate parsing)\n",
                "\n",
                "**Approach:** Refactor to class-based with lazy evaluation to parse once, access multiple times\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Original implementation (from scraper.py)\n",
                "from bs4 import BeautifulSoup\n",
                "import requests\n",
                "import time\n",
                "\n",
                "headers = {\n",
                "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
                "}\n",
                "\n",
                "def fetch_website_contents(url):\n",
                "    \"\"\"Fetches and parses HTML\"\"\"\n",
                "    response = requests.get(url, headers=headers)\n",
                "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
                "    title = soup.title.string if soup.title else \"No title found\"\n",
                "    if soup.body:\n",
                "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
                "            irrelevant.decompose()\n",
                "        text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
                "    else:\n",
                "        text = \"\"\n",
                "    return (title + \"\\n\\n\" + text)[:2_000]\n",
                "\n",
                "def fetch_website_links(url):\n",
                "    \"\"\"Fetches and parses HTML again (inefficient)\"\"\"\n",
                "    response = requests.get(url, headers=headers)\n",
                "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
                "    links = [link.get(\"href\") for link in soup.find_all(\"a\")]\n",
                "    return [link for link in links if link]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Optimized class-based implementation\n",
                "class WebsiteScraper:\n",
                "    \"\"\"\n",
                "    Parse once, access multiple times. Uses @property for lazy evaluation.\n",
                "    Only computes what's accessed, caches results.\n",
                "    \"\"\"\n",
                "    def __init__(self, url):\n",
                "        self.url = url\n",
                "        self.response = requests.get(url, headers=headers)\n",
                "        self.soup = BeautifulSoup(self.response.content, \"html.parser\")\n",
                "        self._title = None\n",
                "        self._content = None\n",
                "        self._links = None\n",
                "    \n",
                "    @property\n",
                "    def title(self):\n",
                "        if self._title is None:\n",
                "            self._title = self.soup.title.string if self.soup.title else \"No title found\"\n",
                "        return self._title\n",
                "    \n",
                "    @property\n",
                "    def content(self):\n",
                "        if self._content is None:\n",
                "            if self.soup.body:\n",
                "                for irrelevant in self.soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
                "                    irrelevant.decompose()\n",
                "                text = self.soup.body.get_text(separator=\"\\n\", strip=True)\n",
                "            else:\n",
                "                text = \"\"\n",
                "            self._content = (self.title + \"\\n\\n\" + text)[:2_000]\n",
                "        return self._content\n",
                "    \n",
                "    @property\n",
                "    def links(self):\n",
                "        if self._links is None:\n",
                "            links = [link.get(\"href\") for link in self.soup.find_all(\"a\")]\n",
                "            self._links = [link for link in links if link]\n",
                "        return self._links\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Performance Comparison\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_url = \"https://example.com\"\n",
                "\n",
                "# Original: 2 HTTP requests, 2 parsing operations\n",
                "start = time.time()\n",
                "content1 = fetch_website_contents(test_url)\n",
                "links1 = fetch_website_links(test_url)\n",
                "time_original = time.time() - start\n",
                "\n",
                "print(f\"Original: {time_original:.4f}s | 2 requests | 2 parses\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Optimized: 1 HTTP request, 1 parse, lazy evaluation\n",
                "start = time.time()\n",
                "scraper = WebsiteScraper(test_url)\n",
                "content2 = scraper.content\n",
                "links2 = scraper.links\n",
                "time_optimized = time.time() - start\n",
                "\n",
                "print(f\"Optimized: {time_optimized:.4f}s | 1 request | 1 parse\")\n",
                "print(f\"Speedup: {time_original / time_optimized:.2f}x faster\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Notes\n",
                "\n",
                "**Use class-based when:**\n",
                "- Multiple operations on same expensive resource (network, parsing)\n",
                "- Need caching/lazy evaluation\n",
                "- Shared state across operations\n",
                "\n",
                "**Use functions when:**\n",
                "- Single, independent operation\n",
                "- No shared state needed\n",
                "\n",
                "**Tradeoff:** More code complexity vs. better performance and reusability\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Implementation Notes\n",
                "\n",
                "**`@property` pattern:** Makes methods accessible as attributes (`scraper.content` vs `scraper.get_content()`)\n",
                "\n",
                "**Lazy evaluation:** Initialize cache as `None`, compute on first access, return cached value thereafter. Only computes what you actually use.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify lazy evaluation: only accessed properties are computed\n",
                "scraper = WebsiteScraper(\"https://example.com\")\n",
                "print(f\"After init - title: {scraper._title is None}, content: {scraper._content is None}, links: {scraper._links is None}\")\n",
                "\n",
                "_ = scraper.title\n",
                "print(f\"After title access - content: {scraper._content is None}, links: {scraper._links is None}\")\n",
                "\n",
                "_ = scraper.content\n",
                "print(f\"After content access - links: {scraper._links is None}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## JavaScript-Ready Scraper: Playwright Implementation\n",
                "\n",
                "**Problem:** `requests` only gets static HTML; fails on JavaScript-rendered sites (React, Vue, etc.)\n",
                "\n",
                "**Solution:** Use Playwright to run a real browser, execute JavaScript, then extract rendered HTML\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"\n",
                "JavaScript-Ready Website Scraper using Playwright\n",
                "\n",
                "Why This Works:\n",
                "- requests: Gets static HTML only (fast, lightweight, but fails on JS-rendered sites)\n",
                "- Playwright: Runs a real browser, executes JavaScript, then extracts rendered HTML\n",
                "  (handles React, Vue, Angular, and other JS frameworks)\n",
                "\n",
                "Tradeoffs:\n",
                "- Playwright: Handles JavaScript, but slower and more resource-intensive\n",
                "- requests: Fast and lightweight, but only works for static sites\n",
                "\n",
                "Use Case - Summarization:\n",
                "This enables scraping modern JavaScript-heavy websites (e.g., openai.com, react apps)\n",
                "for LLM summarization. The rendered content is then passed to the LLM for analysis,\n",
                "enabling summarization of sites that would otherwise return empty or incomplete content.\n",
                "\"\"\"\n",
                "\n",
                "from playwright.sync_api import sync_playwright\n",
                "from bs4 import BeautifulSoup\n",
                "\n",
                "class Website:\n",
                "    def __init__(self, url):\n",
                "        self.url = url\n",
                "        self.title = None\n",
                "        self.text = None\n",
                "    \n",
                "    def fetch(self):\n",
                "        \"\"\"Fetch website using Playwright (handles JavaScript)\"\"\"\n",
                "        with sync_playwright() as p:\n",
                "            browser = p.chromium.launch(headless=True)\n",
                "            page = browser.new_page()\n",
                "            \n",
                "            # Navigate and wait for content\n",
                "            page.goto(self.url, wait_until=\"networkidle\")\n",
                "            \n",
                "            # Get rendered HTML\n",
                "            html = page.content()\n",
                "            self.title = page.title()\n",
                "            \n",
                "            browser.close()\n",
                "        \n",
                "        # Parse with BeautifulSoup (same as before)\n",
                "        soup = BeautifulSoup(html, \"html.parser\")\n",
                "        if soup.body:\n",
                "            for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
                "                irrelevant.decompose()\n",
                "            self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
                "        else:\n",
                "            self.text = \"\"\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Notes\n",
                "\n",
                "**When to use Playwright:**\n",
                "- JavaScript-rendered sites (React, Vue, Angular, SPA)\n",
                "- Dynamic content that loads after page load\n",
                "- Sites that require browser execution\n",
                "\n",
                "**When `requests` is sufficient:**\n",
                "- Static HTML sites\n",
                "- Server-side rendered content\n",
                "- Performance-critical scenarios\n",
                "\n",
                "**Tradeoff:** Browser overhead (slower, more resources) vs. compatibility (handles modern web apps)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.0.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
