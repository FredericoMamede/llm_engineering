{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model Comparison: Local vs Frontier Models\n",
                "\n",
                "**Goal:** Compare different LLM models to understand tradeoffs\n",
                "\n",
                "**Models tested:**\n",
                "- Local (Ollama): llama3.2:1b, llama3.2, deepseek-r1:1.5b\n",
                "- Frontier (OpenAI): gpt-4o-mini\n",
                "\n",
                "**Focus:** Quality, speed, cost, privacy tradeoffs\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "from IPython.display import Markdown, display\n",
                "from openai import OpenAI\n",
                "import time\n",
                "import os\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# Test content (same for all models)\n",
                "test_url = \"https://ollama.com\"\n",
                "headers = {\n",
                "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
                "}\n",
                "\n",
                "# Scrape test content once\n",
                "response = requests.get(test_url, headers=headers)\n",
                "soup = BeautifulSoup(response.content, \"html.parser\")\n",
                "if soup.body:\n",
                "    for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
                "        irrelevant.decompose()\n",
                "    test_content = soup.body.get_text(separator=\"\\n\", strip=True)[:2000]\n",
                "else:\n",
                "    test_content = \"\"\n",
                "\n",
                "print(f\"Test content length: {len(test_content)} chars\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Test Prompt\n",
                "\n",
                "Same prompt for all models to ensure fair comparison.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "system_prompt = \"You are an assistant that provides concise summaries. Respond in markdown.\"\n",
                "user_prompt = f\"Summarize this website content:\\n\\n{test_content}\"\n",
                "\n",
                "messages = [\n",
                "    {\"role\": \"system\", \"content\": system_prompt},\n",
                "    {\"role\": \"user\", \"content\": user_prompt}\n",
                "]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Local Models (Ollama)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
                "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')\n",
                "\n",
                "local_models = {\n",
                "    \"llama3.2:1b\": \"Fastest, smallest (1B params)\",\n",
                "    \"llama3.2\": \"Balanced (3B params)\",\n",
                "    \"deepseek-r1:1.5b\": \"Better reasoning (1.5B params)\"\n",
                "}\n",
                "\n",
                "results_local = {}\n",
                "\n",
                "for model_name, description in local_models.items():\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Testing: {model_name}\")\n",
                "    print(f\"Description: {description}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    try:\n",
                "        start = time.time()\n",
                "        response = ollama.chat.completions.create(\n",
                "            model=model_name,\n",
                "            messages=messages\n",
                "        )\n",
                "        elapsed = time.time() - start\n",
                "        \n",
                "        summary = response.choices[0].message.content\n",
                "        results_local[model_name] = {\n",
                "            \"summary\": summary,\n",
                "            \"time\": elapsed,\n",
                "            \"tokens\": getattr(response.usage, 'total_tokens', 'N/A')\n",
                "        }\n",
                "        \n",
                "        print(f\"Time: {elapsed:.2f}s\")\n",
                "        display(Markdown(summary))\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Error: {e}\")\n",
                "        results_local[model_name] = {\"error\": str(e)}\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Frontier Model (OpenAI) - Optional\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "load_dotenv()\n",
                "openai_key = os.getenv('OPENAI_API_KEY')\n",
                "\n",
                "if openai_key:\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(\"Testing: gpt-4o-mini (Frontier)\")\n",
                "    print(\"Description: OpenAI frontier model, cloud-based\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    try:\n",
                "        openai_client = OpenAI()\n",
                "        start = time.time()\n",
                "        response = openai_client.chat.completions.create(\n",
                "            model=\"gpt-4o-mini\",\n",
                "            messages=messages\n",
                "        )\n",
                "        elapsed = time.time() - start\n",
                "        \n",
                "        summary = response.choices[0].message.content\n",
                "        print(f\"Time: {elapsed:.2f}s\")\n",
                "        print(f\"Tokens: {response.usage.total_tokens}\")\n",
                "        display(Markdown(summary))\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Error: {e}\")\n",
                "else:\n",
                "    print(\"OpenAI API key not found - skipping frontier model comparison\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparison Summary\n",
                "\n",
                "**Key Tradeoffs:**\n",
                "\n",
                "| Model Type | Quality | Cost | Privacy | Speed | Use Case |\n",
                "|------------|---------|------|---------|-------|----------|\n",
                "| Frontier (OpenAI) | High | Paid | Cloud | Fast | Production, quality-critical |\n",
                "| Local (Ollama) | Medium | Free | Local | Variable | Privacy, experimentation, cost-sensitive |\n",
                "\n",
                "**Model Size Tradeoffs:**\n",
                "- Smaller models (1B): Faster, less capable, lower memory\n",
                "- Larger models (3B+): Slower, more capable, higher memory\n",
                "- Reasoning models: Better logic, may be slower\n",
                "\n",
                "**Pattern:** Choose model based on requirements (quality vs cost vs privacy vs speed)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.0.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
